---
title: "GroupE_Classification"
date: "5/29/2019"
output:
  html_document:
    toc: true
    toc_depth: 3
author: "Group E"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, COMMENT = NA, warning = FALSE)
start_time <- Sys.time()
print(paste0('---START--- Starting at ',start_time))
packages_list <- c(
  'caret',
  'corrplot',
  'data.table',
  'DMwR',
  'dplyr',
  'GGally',
  'ggplot2',
  'lubridate',
  'MLmetrics')
for (i in packages_list){
  if(!i%in%installed.packages()){
    install.packages(i, dependencies = TRUE)
    library(i, character.only = TRUE)
    print(paste0(i, ' has been installed'))
  } else {
    print(paste0(i, ' is already installed'))
    library(i, character.only = TRUE)
  }
}
print(paste0('[', round(difftime(Sys.time(),start_time, units = 'secs'),1), 's]: ',
             'All necessary packages installed and loaded'))
```

## 1. Introduction
The dataset provided gathers information related to telemarketing campaigns applied by a banking institution. It is important to mention that a client was contacted a few times in order to see if the product (term deposit) would be or not subscribed.

The goal of the project is to predict by using a classification model if a client will subscribe for a term deposit, having as a target variable a binary parameter with possible outcomes 'yes' or 'no'.

## 2. Data Loading and Preprocessing

Here we load the necessary train and test data.

```{r load}
train = fread("BankCamp_train.csv", stringsAsFactors = T)
test = fread("BankCamp_test.csv", stringsAsFactors = T)
```

Here we switch the order of the factors, to get metrics related to the yes target.

```{r change_factors}
train$y <- factor(train$y, levels = c("yes", "no"))
test$y <- factor(test$y, levels = c("yes", "no"))
```

These are the first few rows of the training data.

```{r head}
head(train)
```

This is the structure of the data.

```{r str}
str(train)
```

This is the summary of the data.

```{r summary}
summary(train)
```

Now we check if there are missing values in any column:

```{r missing}
sapply(train, function(x) sum(is.na(x)))
```

We also check if there are any duplicate values:

```{r duplicated}
any(duplicated(train))
```

Finally we set the types for numerical variables:

```{r numerical}
train[ , which(sapply(train, is.integer)):=lapply(.SD,as.numeric), .SDcols = sapply(train, is.integer)]
test[ , which(sapply(test, is.integer)):=lapply(.SD,as.numeric), .SDcols = sapply(test, is.integer)]
str(train)
```

## 3. Exploratory Data Analysis (EDA)

### Target Variable Distribution

```{r}
sort(summary(train$y), dec=T)/nrow(train)

p1<-ggplot(train, aes(x=y))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+ labs(title = "Distribution of Clients Subscribed")+
  xlab("y")+ ylab("Count")
train[, job:=factor(job, levels=names(sort(summary(train$job), dec=T)))]
levels(train$job)
p1
```

### Numerical Variables

### Correlation Plot

In EDA we first create a correlation plot to see if there is any significant correlation between explanatory variables.

```{r}
numerical <- select_if(train,is.numeric)
corr_plot <- ggcorr(numerical,  label_round=2, label = TRUE)
corr_plot
```

As we can see, no variables have significant correlations, we therefore do not need to transform or drop variables based on multivariate correlation between numeric explanatory variables. 

### Univariate Analysi

```{r}
num_y <- train[, c('age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous','y')]
ggpairs(num_y[,1:8], title = "Distribution & Correlation graph", ggplot2::aes(colour=y, alpha=1/100))
```

### Categorical Variables

Next we check univariate distributions of each variable, to see if the numerical variables are skewed, or if some factors of the categorical variables account for more of the data, than others.

### Univariate Analysis

### Age
#### Age univariate bar chart (count)
```{r}
sort(summary(train$age), dec=T)/nrow(train)
p5<-ggplot(train, aes(x=age))+geom_histogram(fill="dodgerblue4", colour="dodgerblue4", bins=30)+
  theme(axis.text.x = element_text(angle=0))+
  labs(title = "Distribution by Age")+xlab("Age")+ ylab("Count")
levels(train$age)
p5
```

We see the age distribution skewed to the right, with the majority appliactions between 30 and 40 years of age.

### Job
##### Job univariate bar chart (count)
```{r}
sort(summary(train$job), dec=T)/nrow(train)

p1<-ggplot(train, aes(x=job))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=90))+ labs(title = "Distribution by Kind of Job")+
  xlab("Job")+ ylab("Count")
train[, job:=factor(job, levels=names(sort(summary(train$job), dec=T)))]
levels(train$job)
p1
```
We see that a large portion of historic applicants is made up of blue collar and management related jobs.

### Marital
#### Marital univariate bar chart (count)
```{r}
sort(summary(train$marital), dec=T)/nrow(train)

p2<-ggplot(train, aes(x=marital))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+ labs(title = "Distribution by Marital Status")+
  xlab("Marital Status")+ ylab("Count")
train[, marital:=factor(marital, levels=names(sort(summary(train$marital), dec=T)))]
levels(train$marital)
p2
```
Most of the historic applicants were married, with a fraction being divorced.

### Education
##### Education univariate bar chart (count)
```{r}
sort(summary(train$education), dec=T)/nrow(train)

p3<-ggplot(train, aes(x=education))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Distribution by Education Level")+
  xlab("Education Level")+ ylab("Count")
train[, education:=factor(education, levels=names(sort(summary(train$education), dec=T)))]
levels(train$education)
p3
```
Most historic applicants have at least secondary education, making them good candidates from a business point of view.

### Month
#### Month univariate bar chart (count)
```{r}
sort(summary(train$month), dec=T)/nrow(train)
train$month <- factor(train$month, levels = c( "jan", "feb", "mar", "apr","may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"))
p4<-ggplot(train, aes(x=month))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=90))+labs(title = "Distribution by Month")+
  xlab("Month")+ ylab("Count")
levels(train$month)
p4
```

We see spikes of applications in may, sustained throughout the summer and dropping during the rest of the year.

### Loan
#### Loan univariate bar chart (count)
```{r}
sort(summary(train$loan), dec=T)/nrow(train)

p23<-ggplot(train, aes(x=loan))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Loans Acquired")+
  xlab("Loans")+ ylab("Count")
p23
```

### Housing Loan
#### Housing Loan univariate bar chart (count)
```{r}
sort(summary(train$housing), dec=T)/nrow(train)

p24<-ggplot(train, aes(x=housing))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Housing Loan Acquired")+
  xlab("Housing Loan")+ ylab("Count")
p24
```

### Contact 
#### Contact univariate bar chart (count)
```{r}
sort(summary(train$contact), dec=T)/nrow(train)

p24<-ggplot(train, aes(x=contact))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Contact Type")+
  xlab("Contact")+ ylab("Count")
p24
```

### Previous Campaign Outcome 
#### Previous Campaign  univariate bar chart (count)
```{r}
sort(summary(train$poutcome), dec=T)/nrow(train)

p24<-ggplot(train, aes(x=poutcome))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Previous Campaign Outcome")+
  xlab("Outcome")+ ylab("Count")
p24
```

### Bivariate Analysis
In the bivariate analysis, we see what proportion of applicants have subscribed and not subscribed according to each variable.

#### Marital 
```{r}
#Marital + Loan bar chart (count)
p6<-ggplot(train, 
           aes(x = marital, 
               fill = loan)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  labs(title = "Loan Acquisition by Marital Status")+xlab("Marital Status")+ ylab("Count")
p6
```
We see that there seems to be a linear correlation between 

#### Marital + Default bar chart (count)
```{r}
p7<-ggplot(train, 
             aes(x = marital, 
                 fill = default)) + 
    geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  labs(title = "Default by Marital Status")+xlab("Marital Status")+ ylab("Count")
p7
```

#### Marital + y bar chart (percent)
```{r}
mytable <- table(train$marital, train$y)
tab <- as.data.frame(prop.table(mytable, 2))
colnames(tab) <-  c("marital", "y", "perc")

p8<-ggplot(data = tab, aes(x = marital, y = perc, fill = y)) + 
  geom_bar(stat = 'identity', position = 'dodge', alpha = 1) + scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  labs(title = "Term Deposit Subscription by Marital Status")+xlab("Marital Status")+ ylab("Percent")
p8
```

### Education
#### Education + Loan bar chart (count)
```{r}
p9<-ggplot(train, 
           aes(x = education, 
               fill = loan)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  labs(title = "Loan Acquisition by Education Level")+xlab("Education Level")+ ylab("Count")
p9
```

### Education + Default bar chart (count)
```{r}
p10<-ggplot(train, 
           aes(x = education, 
               fill = default)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  labs(title = "Default by Education Level")+xlab("Education Level")+ ylab("Count")
p10
```

#### Education + y bar chart (percent)
```{r}
mytable <- table(train$education, train$y)
tab <- as.data.frame(prop.table(mytable, 2))
colnames(tab) <-  c("education", "y", "perc")

p11<- ggplot(data = tab, aes(x = education, y = perc, fill = y)) + 
  geom_bar(stat = 'identity', position = 'dodge', alpha = 1) + scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
labs(title = "Term Deposit Subscription by Education Level")+xlab("Education Level")+ ylab("Percent")
p11
```

### Job
#### Job + Loan bar chart (count)
```{r}
p12<-ggplot(train, 
           aes(x = job, 
               fill = loan)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+theme(axis.text.x = element_text(angle=90))+
  labs(title = "Job by Loan Status")+xlab("Job")+ ylab("Count")
p12
```

#### Job + Default bar chart (count)
```{r}
p13<-ggplot(train, 
            aes(x = job, 
                fill = default)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+theme(axis.text.x = element_text(angle=90))+
  labs(title = "Default Status by Job")+xlab("Job")+ ylab("Count")
p13
```

#### Job + y bar chart (percent)
```{r}
mytable <- table(train$job, train$y)
tab <- as.data.frame(prop.table(mytable, 2))
colnames(tab) <-  c("job", "y", "perc")

p14<-ggplot(data = tab, aes(x = job, y = perc, fill = y)) + 
  geom_bar(stat = 'identity', position = 'dodge', alpha = 1) + scale_fill_manual(values=c("dodgerblue4","dodgerblue3"))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  labs(title = "Term Deposit Subscription by Job")+xlab("Job")+ ylab("Percent")
p14
```

### Balance
#### Balance + education box plot
```{r}
p15<-ggplot(train, 
       aes(x = education, 
           y = balance)) + geom_line(size = 1.5, 
                                     color = "lightgrey") +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4") +
  labs(title = "Balance by Education Level")+xlab("Education")+ ylab("Balance")
p15
```

### Balance + education scatter plot
```{r}
p16<-ggplot(train, 
            aes(y = balance, 
                x = education)) +  
  geom_jitter(color = "dodgerblue4") + 
  labs(title = "Balance Distribution by Education Level")+theme(axis.text.x = element_text(angle=90))+
  xlab("Education")+ ylab("Balance")
p16
```

#### Balance + job box plot
```{r}
p17<-ggplot(train, 
            aes(x = job, 
                y = balance)) + geom_line(size = 1.5, 
                                          color = "lightgrey") +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4") +
  labs(title = "Balance by Kind of Job")+theme(axis.text.x = element_text(angle=90))+
  xlab("Job")+ ylab("Balance")
p17
```

#### Balance + job scatter plot
```{r}
p18<-ggplot(train, 
       aes(y = balance, 
           x = job)) +  
  geom_jitter(color = "dodgerblue4") + 
  labs(title = "Balance Distribution by Kind of Job")+theme(axis.text.x = element_text(angle=90))+
  xlab("Job")+ ylab("Balance")
p18
```

#### Duration + y
```{r}
p19<-ggplot(train, aes(x=as.factor(y), y=duration)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Call Duration by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Call Duration (sec.)")
p19
```

#### Pdays + y
```{r}
p20<-ggplot(train, aes(x=as.factor(y), y=pdays)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Days from last contact by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Days from last contact")
p20
```

#### Previous + y
```{r}
p21<-ggplot(train, aes(x=as.factor(y), y=previous)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Contacts before this Campaign by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Contacts before this campaign")
p21
```

#### Campaign + y
```{r}
p22<-ggplot(train, aes(x=as.factor(y), y=campaign)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Contacts during this Campaign by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Contacts during this campaign")
p22
```
### Coefficient of Variation

The coefficient of variation is a dimensionless meassure of dispersion in data, the lower the value the less dispersion a feature has.

```{r}
numeric_variables<-names(numerical)
sd_numeric_variables<-sapply(train[,numeric_variables, with=F], sd)
cv_numeric_variables<-sd_numeric_variables/colMeans(train[,numeric_variables, with=F])

ggplot(data.table(var=names(cv_numeric_variables),cv=cv_numeric_variables),
       aes(var,fill=cv))+geom_bar()+coord_polar()+scale_fill_gradient(low='white', high = 'dodgerblue4')

```
Viewing variables with less than a 0.05 coefficient of variation. There are none.

```{r}
cv_numeric_variables[cv_numeric_variables < 0.05]
```
### Linear Dependencies
Caret uses QR decomposition to enumerate sets of linear combinations and therefore, remove them, however there are none.

```{r}
lc<-findLinearCombos(train[, ..numeric_variables])
lc
```

## 4. Baseline

### Setting up Preprocessing: Centering, Scaling and performing a Yeo Johnson transformation
To reduce the weight/importance of certain features due to their scale, we center them and scale them. We also reduce skewness by performing a Yeo Johnson transformation. This combination will be performed on each model training.
```{r}
pp<-c("center", "scale", "YeoJohnson")
preProcess(train, method =pp)
```

### Dummy Encoding
We dummy encode the factor variables to be able to run different models on the data.

```{r dummy}
dummy<-dummyVars(formula= ~., data = train[, -"y"], sep = "_")
final_train<-data.table(predict(dummy, newdata = train[, -"y"]))
final_train$y<-train$y
final_test<-data.table(predict(dummy, newdata = test))
head(final_train)
```

Now we define our training function, it will perform a 5 fold cross validation on 80% of the training data and then a final validation on the remaining 20% of the data that acts as a holdout. It also does the aforementioned preprocessing.

```{r}
train_val<- function(train_dt, model, sampling){
  tc<-trainControl(
    method = "cv",
    number=5,
    savePredictions = TRUE,
    classProbs=TRUE,
    summaryFunction = prSummary)

  trainIndex <- createDataPartition(train_dt$y, p = .8, list = FALSE)
  model_train <- train_dt[ trainIndex,]
  holdout  <- train_dt[-trainIndex,]
  
  if(!missing(sampling)){
    if(sampling == 'over'){
      model_train<-upSample(x = model_train[, -"y"],y = model_train$y, yname="y")
    }
    else if(sampling == 'under'){
      model_train<-downSample(x = model_train[, -"y"],y = model_train$y, yname="y")
    }
    else {
      model_train<-SMOTE(y ~ ., data  = model_train) 
    }
  }
  
  ini<-now()
  model<- train(y~ ., data = model_train, method = model, metric="AUC", trControl=tc, preProcess=pp)
  message("Cross Validation Scores having Yes as the positive class")
  message(model$results)
  message("Train + Predict time:")
  message(now()-ini)
  
  predicted = predict(model, newdata = holdout)
  
  message("Holdout Scores")
  message(confusionMatrix(table(predicted, holdout$y), positive="yes", mode="everything"))
  
  return(model)
}
```

Now we will run a simple logistic regression as a baseline, this will serve as a way of knowing if our feature engineering steps improve performance.

```{r baseline}
lm <- train_val(final_train, "glm")
```

So our Sensitivity or Recall is very low based on the "yes" class, meaning that we correctly classify a really small set of the "yes" cases.

Now we run a random forest as an alternative baseline.

```{r}
rf <- train_val(final_train, "ranger")
```

## 5. Feature Engineering
Here we merge the two datasets to perform multiple feature engineering steps
```{r feature engineering}
y <- final_train$y
final_train$y <- NULL
merged_df <- rbind(final_train, final_test)
```

#### Days to end of month assuming all the months have 30 days
```{r}
merged_df$days_to_end_of_month <- 30 - merged_df$day
```
#### Balance General Status -> 1 if positive, 0 if negative or 0
```{r}
positive_balance <- function(number){
  is_positive = 0
  if(number>0){is_positive <- 1}
  return(is_positive)
}
merged_df$balance_general_status <- as.numeric(lapply(merged_df$balance,  FUN = positive_balance))
```

#### Quantity of loans 0, 1 or 2 (housing and personal)
```{r}
merged_df$quantity_loans <- merged_df$housing_yes+merged_df$loan_yes
```

####Days_binned_weeks
```{r}
week_type <- function(day){
  week_num=0
  if(day < 7){
    week_num <-1
  }else if(day< 15){
    week_num <-2
  }else if(day<22){
    week_num <-3
  }else if(day<30){
    week_num<-4
  }else{week_num <-5}
  return(week_num)
}
merged_df$week <- as.factor(as.numeric(lapply(merged_df$day, FUN = week_type)))
dmy<-dummyVars("~.",data = merged_df)
merged_df<-data.table(predict(dmy, newdata = merged_df))
```

#### Season Quarters
```{r}
merged_df$Q1 <- merged_df$month_jan+merged_df$month_feb+merged_df$month_mar
merged_df$Q2 <- merged_df$month_apr+merged_df$month_may+merged_df$month_jun
merged_df$Q3 <- merged_df$month_jul+merged_df$month_aug+merged_df$month_sep
merged_df$Q4 <- merged_df$month_oct+merged_df$month_nov+merged_df$month_dec

train_featured <- merged_df[1:nrow(train),]
train_featured$y <- y
test_featured <- merged_df[(nrow(train)+1):nrow(merged_df),]
```

## 6. Modeling

### Logistic Regression
```{r, message=FALSE}
lm_over <- train_val(train_featured, "glm","over")
lm_under <- train_val(train_featured, "glm","under")
lm_S <- train_val(train_featured, "glm","SMOTE")
```

### Random Forest
```{r, message=FALSE}
rf_over <- train_val(train_featured, "ranger","over")
rf_under <- train_val(train_featured, "ranger","under")
rf_S <- train_val(train_featured, "ranger","SMOTE")
```

### XGBoosting Tree
```{r, message=FALSE}
xgb_over <- train_val(train_featured, "xgbTree","over")
xgb_under <- train_val(train_featured, "xgbTree","under")
xgb_S <- train_val(train_featured, "xgbTree","SMOTE")
```

### Regression with regularization
```{r, message=FALSE}
glmnet_over <- train_val(train_featured, "glmnet","over")
glmnet_under <- train_val(train_featured, "glmnet","under")
glmnet_S <- train_val(train_featured, "glmnet","SMOTE")
```

### Results
```{r}
results <- resamples(list(
    LMO = lm_over,
    LMU = lm_under,
    LMS = lm_S,
    RFO = rf_over,
    RFU = rf_under,
    RFS = rf_S,
    XGBO = xgb_over,
    XGBU = xgb_under,
    XGBS = xgb_S,
    GLO = glmnet_over,
    GLU = glmnet_under,
    GLS = glmnet_S)
)

summary(results)
```

### Plot of Results
```{r}
bwplot(results, layout = c(2, 1), scales = list(relation="free"))
```
Based on this, we realize that the best performing model is the Random Forest with SMOTE sampling.

## 7. Hyperparameter Tuning
Finally we perform a cross validated, randomized grid search on the chosen random forest in order to define the final model, optimizing for Recall.

```{r tuning}
grid<-expand.grid(
  mtry=c(16,32,63),
  splitrule=c('extratrees', 'gini'),
  min.node.size=c(1,3,5)
)

tc<-trainControl(
  method = "repeatedcv",
  number=5,
  repeats = 2,
  savePredictions = TRUE,
  summaryFunction = prSummary,
  classProbs=TRUE,
  search = "random"
)

ini<-now()

model_train<-SMOTE(y ~ ., data  = train_featured) 

grid_rf <- train(
  y ~ .,
  data = model_train,
  method = "ranger",
  num.trees=500,
  tuneGrid = grid,
  trControl = tc,
  metric = "Recall",
  tuneLength = 5,
  verbose = TRUE,
  importance = "impurity",
  preProcess=pp
)

print(now()-ini)

grid_rf
```

```{r}
grid_rf$bestTune
```

```{r}
grid_rf$finalModel
```

```{r}
plot(grid_rf)
```

```{r}
plot(varImp(grid_rf), top = 20)
```

## 8. Predictions
We predict on the test set, add the predictions to it and create our predictions file.
```{r predictions}
predicted <- predict(grid_rf, newdata = test_featured)
df_pred<-cbind(test, predicted)
head(df_pred)
```

```{r}
fwrite(df_pred[,c("predicted")],"predictions.csv")
```

## 9. Conclusions

We conclude...
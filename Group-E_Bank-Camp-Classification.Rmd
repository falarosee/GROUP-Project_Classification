---
title: "Bank Campaign Classification"
date: "5/29/2019"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
author: "Group E"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE)
set.seed(1000)
packages_list <- c(
  'caret',
  'corrplot',
  'data.table',
  'DMwR', #SMOTE
  'dplyr',
  'GGally', #Corrplot
  'ggplot2',
  'glmnet', #Lasso regularization
  'lubridate',
  'MLmetrics',
  'plotly', #Interactive correlation
  'pROC') #ROC Graph
for (i in packages_list){
  if(!i%in%installed.packages()){
    install.packages(i, dependencies = TRUE)
    library(i, character.only = TRUE)
    print(paste0(i, ' has been installed'))
  } else {
    print(paste0(i, ' is already installed'))
    library(i, character.only = TRUE)
  }
}
print(paste0('[', round(difftime(Sys.time(),start_time, units = 'secs'),1), 's]: ',
             'All necessary packages installed and loaded'))
```

## 1. Introduction

The dataset provided gathers information related to a telemarketing campaign applied by a banking institution. Throughout the campaign, clients were contacted multiple times in regard to a term deposit subscription offer. 

The goal of this project is to predict if a client will subscribe to the term deposit offer. To this end a classification analysis was conducted. The term deposit target variable is labelled 'y', and has the possible outcomes 'yes' or 'no'. 

## 2. Data Loading and Preprocessing

The first part of this report revolves around understanding the data. Here, the data quality is evaluated and some basic cleaning and pre-processing is undertaken in order to prepare the data for the baseline model which will act as a benchmark for all future data transformation in the feature engineering process. Within this pipe-line, pre-processing involves anything that can be done to get the data ready for a baseline model that does not require an EDA (Exploratory Data Analysis). This typically involves loading the data, merging it, looking for NA's, looking for missing values, dropping meaningless variables and correcting variable types. 

First, we load the necessary train and test data.

```{r load}
train = fread("BankCamp_train.csv", stringsAsFactors = T)
test = fread("BankCamp_test.csv", stringsAsFactors = T)
```

Here we switch the order of the factors to get metrics related to the "yes" target.

```{r change_factors}
train$y <- factor(train$y, levels = c("yes", "no"))
test$y <- factor(test$y, levels = c("yes", "no"))
```

Finally we set the types for numerical variables:

```{r numerical}
train[ , which(sapply(train, is.integer)):=lapply(.SD,as.numeric), .SDcols = sapply(train, is.integer)]
test[ , which(sapply(test, is.integer)):=lapply(.SD,as.numeric), .SDcols = sapply(test, is.integer)]
```

### 2.1 Initial Exploration {.tabset .tabset-fade .tabset-pills}
***
We explore the first few rows, structure and summary of the training data to see if there are any inconsistencies.

We see there are neither inconsistent missing values nor duplicates in the set. However, we see that the "pdays" variable contains '-1' values, which we assume indicates that the information is not applicable to the client.

#### First Rows
```{r head}
head(train)
```

#### Structure
```{r str}
str(train)
```

#### Summary
```{r summary}
summary(train)
```

#### Misssing
```{r missing}
sapply(train, function(x) sum(is.na(x)))
```

#### Duplicates
```{r duplicated}
any(duplicated(train))
```

## 3. Exploratory Data Analysis (EDA)

In the first step, we explore the distribution of the variables, in order to gain insights of the data and inform decisions on the later feature engineering and modelling process.

### 3.1 Target Variable Distribution

```{r}
sort(summary(train$y), dec=T)/nrow(train)
p0<-ggplot(train, aes(x=y))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4", label = TRUE)+
  theme(axis.text.x = element_text(angle=0))+ labs(title = "Distribution of Clients Subscribed")+
  xlab("y")+ ylab("Count")
train[, job:=factor(job, levels=names(sort(summary(train$job), dec=T)))]
levels(train$job)
p0
```

<hr />

We see that the target variable is heavily unbalanced, with over 85% of the values denoting 'no'. In the business context this is explained by the fact that the term deposit subscription is not targeted towards the majority of clients. 

For later modelling and model evaluation, this poses a challenge, as the model is likely to achieve a high accuracy by purely training on and predicting 'no' targets, despite having a very low sensitivity score (namely being poor at correctly indentifying true positive targets). 

### 3.2 Numerical Variable Distributions

#### 3.21 Numeric Correlation Plots

Next, a correlation plot is constructed to see if there is any significant correlation between numeric explanatory variables.

```{r}
numerical <- select_if(train,is.numeric)
corr_plot <- ggcorr(numerical,  label_round=2, label = TRUE)
corr_plot
```

As we can see, no variables have significant correlations, apart from "previous" and "pdays". We therefore do not need to spend a lot of time on transforming or dropping variables purely based on multivariate correlation between numeric explanatory variables. 

Here is an interactive version of the correlation matrix.

```{r}
plot_ly(x = colnames(train),
        y=colnames(train), 
        z = cor(numerical), colors = colorRamp(c("dodgerblue4", "brown2")),
        type = "heatmap") %>% layout(title="Interactive Correlation Matrix")
```


#### 3.22 Numerical Variables Plot

```{r, fig.height=12, fig.width=16}
num_y <- train[, c('age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous','y')]
ggpairs(num_y[,1:8], title = "Distribution & Correlation graph", ggplot2::aes(colour=y, alpha=1/100))
```

In the above graph we see how the numeric variables relate to each other and to the target variable. We can see through the diagram diagonal, that their might be trends within the continuous distributions of the explanatory variables that match up more with certain 'yes', or certain 'no' factors and can explore using these trends to bin variables in the feature engineering phase.

We see that many of the variables are skewed, so we will consider normalizing the scales of the numeric variables at a later stage. 
<hr />

### 3.3 Categorical Variable Distribution {.tabset .tabset-fade .tabset-pills}
***
In this subsection, we explore the categorical variable distributions to inform on later feature engineering decisions.

#### 3.31 Job Distribution
```{r}
sort(summary(train$job), dec=T)/nrow(train)
p1<-ggplot(train, aes(x=job))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=90))+ labs(title = "Distribution by Kind of Job")+
  xlab("Job")+ ylab("Count")
train[, job:=factor(job, levels=names(sort(summary(train$job), dec=T)))]
levels(train$job)
p1
```

We see that a large portion of potential targets is made up of blue collar and management related jobs.

#### 3.32 Marital Status Distribution
```{r}
sort(summary(train$marital), dec=T)/nrow(train)
p2<-ggplot(train, aes(x=marital))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+ labs(title = "Distribution by Marital Status")+
  xlab("Marital Status")+ ylab("Count")
train[, marital:=factor(marital, levels=names(sort(summary(train$marital), dec=T)))]
levels(train$marital)
p2
```

Most of the historic potential targets were married, with a fraction being divorced.

#### 3.33 Education Level Distribution
```{r}
sort(summary(train$education), dec=T)/nrow(train)
p3<-ggplot(train, aes(x=education))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Distribution by Education Level")+
  xlab("Education Level")+ ylab("Count")
train[, education:=factor(education, levels=names(sort(summary(train$education), dec=T)))]
levels(train$education)
p3
```

Most historic potential targets have at least secondary education, making them good candidates from a business point of view.

#### 3.34 Month Distribution
```{r}
sort(summary(train$month), dec=T)/nrow(train)
train$month <- factor(train$month, levels = c( "jan", "feb", "mar", "apr","may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"))
p4<-ggplot(train, aes(x=month))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=90))+labs(title = "Distribution by Month")+
  xlab("Month")+ ylab("Count")
levels(train$month)
p4
```

We see spikes of subscriptions in May, sustained throughout the summer and dropping during the rest of the year.

#### 3.35 Loan Acquisition Distribution
```{r}
sort(summary(train$loan), dec=T)/nrow(train)
p5<-ggplot(train, aes(x=loan))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Loans Acquired")+
  xlab("Loans")+ ylab("Count")
p5
```

We see that the majority of potential targets do not have a loan.

#### 3.36 Housing Loan Acquisition Distribution
```{r}
sort(summary(train$housing), dec=T)/nrow(train)
p6<-ggplot(train, aes(x=housing))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Housing Loan Acquired")+
  xlab("Housing Loan")+ ylab("Count")
p6
```

The housing loans however, are more balanced.

#### 3.37 Contact Type Distribution
```{r}
sort(summary(train$contact), dec=T)/nrow(train)
p7<-ggplot(train, aes(x=contact))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Contact Type")+
  xlab("Contact")+ ylab("Count")
p7
```

Most potential applicants can be reached by cellular phone, a significant portion however cannot be reached, as they are unknown. 

#### 3.38 Previous Campaign Outcome Distribution
```{r}
sort(summary(train$poutcome), dec=T)/nrow(train)
p8<-ggplot(train, aes(x=poutcome))+geom_bar(stat='count',fill="dodgerblue4", colour="dodgerblue4")+
  theme(axis.text.x = element_text(angle=0))+labs(title = "Previous Campaign Outcome")+
  xlab("Outcome")+ ylab("Count")
p8
```

In this graph most values are unknown, for this reason the variable has low explanatory power for the model.

### 3.4 Bivariate Analysis {.tabset .tabset-fade .tabset-pills}
***

In the bivariate analysis, we can see what proportion of clients have subscribed and not subscribed according to each variable. We further explore trends amongst the explanatory variables, to inform further feature creation.

#### 3.41 Marital 
```{r}
p9<-ggplot(train, 
           aes(x = marital, 
               fill = y)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue3", "dodgerblue4"))+
  labs(title = "Term Deposit Subscription by Marital Status")+xlab("Marital Status")+ ylab("Count")
p9
```

We see that the proportion of married clients that seem to go for a term deposit subscription is smaller than single or divorced people.

#### 3.42 Education
```{r}
p10<-ggplot(train, 
           aes(x = education, 
               fill = y)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue3", "dodgerblue4"))+
  labs(title = "Term Deposit Subscription by Education Level")+xlab("Education Level")+ ylab("Count")
p10
```

Tertiary education people seems to be more tend to go for a term deposit subscription, than primary or secondary.

#### 3.43 Job + Term Deposit bar chart (count)
```{r}
p11<-ggplot(train, 
           aes(x = job, 
               fill = y)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue3", "dodgerblue4"))+ theme(axis.text.x = element_text(angle=90))+
  labs(title = "Term Deposit Subscription by Job")+xlab("Job")+ ylab("Count")
p11
```

It seems that student clients go for a term deposit subscription more than clients working or retired.

#### 3.44 Age + Term Deposit bar chart (count)
```{r}
p12<-ggplot(train, 
           aes(x = age, 
               fill = y)) + 
  geom_bar(position = "dodge")+ scale_fill_manual(values=c("dodgerblue3", "dodgerblue4"))+ theme(axis.text.x = element_text(angle=90))+
  labs(title = "Term Deposit Subscription by Age")+xlab("Age")+ ylab("Count")
p12
```

It seems that middle-aged clients, from 30 to 35 years, go for a term deposit subscription more than younger and older clients.

### 3.5 Outliers Analysis {.tabset .tabset-fade .tabset-pills}
***

In the following section we analyze the distribution of the numerical explanatory variables according to their response to the term deposit subscription. 

#### 3.51 Call Duration
```{r}
p13<-ggplot(train, aes(x=as.factor(y), y=duration)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Call Duration by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Call Duration (sec.)")
p13
```

#### 3.52 Days form last contact
```{r}
p14<-ggplot(train, aes(x=as.factor(y), y=pdays)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Days from last contact by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Days from last contact")
p14
```

#### 3.53 Contacts before this Campaign
```{r}
p15<-ggplot(train, aes(x=as.factor(y), y=previous)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Contacts before this Campaign by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Contacts before this campaign")
p15
```

#### 3.54 Contacts during this Campaign
```{r}
p16<-ggplot(train, aes(x=as.factor(y), y=campaign)) +
  geom_boxplot(fill="dodgerblue3", color="dodgerblue4")+
  theme_classic()+labs(title = "Contacts during this Campaign by Term Deposit Subscription")+xlab("Term deposit")+
  ylab("Contacts during this campaign")
p16
```

### 3.6 Coefficient of Variation
The coefficient of variation is a dimensionless meassure of dispersion in data, the lower the value the less dispersion a feature has.

```{r}
numeric_variables<-names(numerical)
sd_numeric_variables<-sapply(train[,numeric_variables, with=F], sd)
cv_numeric_variables<-sd_numeric_variables/colMeans(train[,numeric_variables, with=F])

ggplot(data.table(var=names(cv_numeric_variables),cv=cv_numeric_variables),
       aes(var,fill=cv))+geom_bar()+coord_polar()+scale_fill_gradient(low='white', high = 'dodgerblue4')

```
Viewing variables with less than a 0.05 coefficient of variation. There are none.

```{r}
cv_numeric_variables[cv_numeric_variables < 0.05]
```

### 3.7 Linear Dependencies
Caret finds sets of linear combinations to remove them, however there are none.

```{r}
lc<-findLinearCombos(train[, ..numeric_variables])
lc
```

## 4. Baseline

### 4.1 Setting up Preprocessing: Centering, Scaling and performing a Yeo Johnson transformation
To reduce the weight/importance of certain features due to their scale, we center them and scale them. We also reduce skewness by performing a Yeo Johnson transformation since it can handle negative values. This combination will be performed on each model training.

```{r}
pp<-c("center", "scale", "YeoJohnson")
preProcess(train, method =pp)
```

### 4.2 Dummy Encoding
We dummy encode the factor variables to be able to run different models on the data.

```{r dummy}
dummy<-dummyVars(formula= ~., data = train[, -"y"], sep = "_")
final_train<-data.table(predict(dummy, newdata = train[, -"y"]))
final_train$y<-train$y
final_test<-data.table(predict(dummy, newdata = test))
```

### 4.3 Train Function
Now we define our training function, it will perform a 5 fold cross validation on 80% of the training data and then a final validation on the remaining 20% of the data that acts as a holdout. It also does the aforementioned preprocessing.

```{r}
train_val<- function(train_dt, model, sampling){
  tc<-trainControl(
    method = "cv",
    number=5,
    savePredictions = TRUE,
    classProbs=TRUE,
    summaryFunction = prSummary)

  trainIndex <- createDataPartition(train_dt$y, p = .8, list = FALSE)
  model_train <- train_dt[ trainIndex,]
  holdout  <- train_dt[-trainIndex,]
  
  if(!missing(sampling)){
    if(sampling == 'over'){
      model_train<-upSample(x = model_train[, -"y"],y = model_train$y, yname="y")
    }
    else if(sampling == 'under'){
      model_train<-downSample(x = model_train[, -"y"],y = model_train$y, yname="y")
    }
    else {
      model_train<-SMOTE(y ~ ., data  = model_train) 
    }
  }
  
  ini<-now()
  model<- train(y~ ., data = model_train, method = model, metric="Recall", trControl=tc, preProcess=pp)
  message("Cross Validation Scores having Yes as the positive class")
  message(model$results)
  message("Train + Predict time:")
  message(now()-ini)
  
  predicted = predict(model, newdata = holdout)
  
  print("Holdout Scores")
  print(confusionMatrix(table(predicted, holdout$y), positive="yes", mode="everything"))
  
  return(model)
}
```

### 4.4 Logistic Regression
Now we will run a simple logistic regression as a baseline, this will serve as a way of knowing if our feature engineering steps improve performance.

```{r baseline}
lm <- train_val(final_train, "glm")
```

So our Sensitivity or Recall is very low based on the "yes" class, meaning that we correctly classify a really small set of the "yes" cases.

## 5. Feature Engineering
Here we merge the two datasets to perform multiple feature engineering steps

```{r feature engineering}
y <- final_train$y
final_train$y <- NULL
merged_df <- rbind(final_train, final_test)
```

### 5.1 Days to end of month assuming all the months have 30 days
We create a variable that measures days to end of month, to see if the date contacted could have any explanatory value in our prediction.

```{r}
merged_df$days_to_end_of_month <- 30 - merged_df$day
```

### 5.2 Balance General Status -> 1 if positive, 0 if negative or 0
We booleanize the 'balance' variable, to make the distinction between having a balance and not, more explicit.

```{r}
positive_balance <- function(number){
  is_positive = 0
  if(number>0){is_positive <- 1}
  return(is_positive)
}
merged_df$balance_general_status <- as.numeric(lapply(merged_df$balance,  FUN = positive_balance))
```

### 5.3 Quantity of loans 0, 1 or 2 (housing and personal)
We create a quantity of loans category, since having a loan or more can make a difference in the decision of a potential client.

```{r}
merged_df$quantity_loans <- merged_df$housing_yes+merged_df$loan_yes
```

### 5.4 Days_binned_weeks
We bin days to weeks incase there is correlation between target and which week the target was contacted. 
```{r}
week_type <- function(day){
  week_num=0
  if(day < 7){
    week_num <-1
  }else if(day< 15){
    week_num <-2
  }else if(day<22){
    week_num <-3
  }else if(day<30){
    week_num<-4
  }else{week_num <-5}
  return(week_num)
}
merged_df$week <- as.factor(as.numeric(lapply(merged_df$day, FUN = week_type)))
dmy<-dummyVars("~.",data = merged_df)
merged_df<-data.table(predict(dmy, newdata = merged_df))
```

### 5.5 Season Quarters
In the EDA we have seen that trends differ to seasons. We therefore bin for seasons.

```{r}
merged_df$Q1 <- merged_df$month_jan+merged_df$month_feb+merged_df$month_mar
merged_df$Q2 <- merged_df$month_apr+merged_df$month_may+merged_df$month_jun
merged_df$Q3 <- merged_df$month_jul+merged_df$month_aug+merged_df$month_sep
merged_df$Q4 <- merged_df$month_oct+merged_df$month_nov+merged_df$month_dec

train_featured <- merged_df[1:nrow(train),]
train_featured$y <- y
test_featured <- merged_df[(nrow(train)+1):nrow(merged_df),]
```

## 6. Modeling

Here we define a number of models, to see which one will preform the best.

### 6.1 Logistic Regression {.tabset .tabset-fade .tabset-pills}
***

#### LR Over
```{r, message=FALSE}
lm_over <- train_val(train_featured, "glm","over")
```

#### LR Under
```{r, message=FALSE}
lm_under <- train_val(train_featured, "glm","under")
```

#### LR SMOTE
```{r, message=FALSE}
lm_S <- train_val(train_featured, "glm","SMOTE")
```

### 6.2 Random Forest {.tabset .tabset-fade .tabset-pills}
***

#### RF Over
```{r, message=FALSE}
rf_over <- train_val(train_featured, "ranger","over")
```

#### RF Under
```{r, message=FALSE}
rf_under <- train_val(train_featured, "ranger","under")
```

#### RF SMOTE
```{r, message=FALSE}
rf_S <- train_val(train_featured, "ranger","SMOTE")
```

### 6.3 XGBoosting Tree {.tabset .tabset-fade .tabset-pills}
***

#### XGB Over
```{r, message=FALSE}
xgb_over <- train_val(train_featured, "xgbTree","over")
```

#### XGB Under
```{r, message=FALSE}
xgb_under <- train_val(train_featured, "xgbTree","under")
```

#### XGB SMOTE
```{r, message=FALSE}
xgb_S <- train_val(train_featured, "xgbTree","SMOTE")
```

### 6.4 Regression with Lasso regularization {.tabset .tabset-fade .tabset-pills}
***

#### GLR Over
```{r, message=FALSE}
glmnet_over <- train_val(train_featured, "glmnet","over")
```

#### GLR Under
```{r, message=FALSE}
glmnet_under <- train_val(train_featured, "glmnet","under")
```

#### GLR SMOTE
```{r, message=FALSE}
glmnet_S <- train_val(train_featured, "glmnet","SMOTE")
```

### 6.5 Results

We print the results to compare the different models.

```{r}
results <- resamples(list(
    LMO = lm_over,
    LMU = lm_under,
    LMS = lm_S,
    RFO = rf_over,
    RFU = rf_under,
    RFS = rf_S,
    XGBO = xgb_over,
    XGBU = xgb_under,
    XGBS = xgb_S,
    GLO = glmnet_over,
    GLU = glmnet_under,
    GLS = glmnet_S)
)

summary(results)
```

### 6.6 Plot of Results
We plot the results to get a deeper understanding which is our best model.

```{r, fig.height=14, fig.width=16}
bwplot(results, layout = c(2, 1), scales = list(relation="free"))
```

Based on this, we realize that the best performing model is the Random Forest with SMOTE sampling.

## 7. Hyperparameter Tuning
Finally we perform a cross validated, randomized grid search on the chosen random forest in order to define the final model, optimizing for Recall.

```{r tuning}
grid<-expand.grid(
  mtry=c(16,32,63),
  splitrule=c('extratrees', 'gini'),
  min.node.size=c(1,3,5)
)

tc<-trainControl(
  method = "cv",
  number=5,
  savePredictions = TRUE,
  summaryFunction = prSummary,
  classProbs=TRUE,
  search = "random"
)

ini<-now()

trainIndex <- createDataPartition(train_featured$y, p = .95, list = FALSE)
model_train <- train_featured[ trainIndex,]
holdout  <- train_featured[-trainIndex,]

model_train<-SMOTE(y ~ ., data  = model_train) 

grid_rf <- train(
  y ~ .,
  data = model_train,
  method = "ranger",
  num.trees=500,
  tuneGrid = grid,
  trControl = tc,
  metric = "Recall",
  tuneLength = 5,
  verbose = TRUE,
  importance = "impurity",
  preProcess=pp
)

print(now()-ini)

grid_rf
```

### 7.1 Final model

And this is the final model:

```{r}
grid_rf$finalModel
```

### 7.2 Plot of results during Grid Search
Here we can see the Recall of the model accross the grid search process.

```{r}
plot(grid_rf)
```

### 7.3 Variable Importance
Here we see the variable importance for the final model.

```{r}
plot(varImp(grid_rf), top = 20)
```

### 7.4 Results on Holdout after Grid Search

#### 7.41 ROC Curve

```{r}
predicted <- predict(grid_rf, newdata = holdout)

roc_curve<-roc(response=as.numeric(factor(holdout$y == "yes")), predictor=as.numeric(factor(predicted == "yes")))
plot.roc(roc_curve, main='ROC on Holdout');grid()
```

#### 7.42 Confusion Matrix

```{r}
print(confusionMatrix(table(predicted, holdout$y), positive="yes", mode="everything"))
plot(table(predicted=predicted, real=holdout$y))
```

## 8. Predictions
We predict on the test set, add the predictions to it and create our predictions file.

```{r predictions}
predicted <- predict(grid_rf, newdata = test_featured)
df_pred<-cbind(test, predicted)
fwrite(df_pred[,c("predicted")],"predictions.csv")
```

## 9. Conclusions

We conclude that after extensive feature exploration, feature engineering, and modelling, the best preforming model is a Random Forrest with SMOTE sampling and the parameters showed above. The final sensitivity score is 0.96. 

The following are the main considerations and conclusions that we can extract from each part of the process:

* Exploratory Data Analysis

The initial dataset was highly unbalanced where, for our target variable (term deposit subscrption), 88% of the labels correspond to class NO. Considering this, we tested different sample techniques (oversampling, undersampling and SMOTE) to understan which technique performs better.

A Shiny Dashboard was built to present some plots and basic data, in order to understand the data given in a easier way. R gives this kind of useful tools to manage in a more visual and interactive way all kind of data. 

* Feature Engineering

We created a set of additional features based on the initial dataset. From these new features we can see that for the final model Days to end of month, Quantity of loans, Q3 and Balance General Status, among others were relevant. 

Considering that the Random forest algorithm automatically performs feature selection, we can see from the plot above that the most remarkable variable is duration.

Our final feature set remains the same beacuse there was no high significant correlation, low score of dispersion or no linear dependencies between all the features. 

* Modelling

We assume that having a false positive error brings a higher cost than a false negative; so in this case we base our results on the sensitivity metric (True positive rate).

After running a Logistc Regression as baseline model and included new features, we tested 4 different classification techniques:

* Logistic Regression
* Logistic Regression with Regularization (LASSO)
* Random Forest
* XG Boost

The best Recall score was from the Random Forest model so a cross validated, randomized grid search was applied in order to define the final model and optimize Recall. Different tuning parameters were tested given the best outcome the following ones:  
* 'min.node.size'=1 
* 'mtry' = 32 
* 'splitrule' = extratrees 
* 'min.node.size' = 1

The results of the final model were presented with an ROC curve and a plotted confusion matrix.

Finally, the target variable was predicted and on the test set and a .csv file was created to store all the data.
